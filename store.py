import os
import fitz  # PyMuPDF
import pdfplumber
import psycopg2
import openai
import json
from dotenv import load_dotenv
import google.generativeai as genai
from collections import defaultdict
from docx import Document
import pytesseract
from PIL import Image
import io
import numpy as np
import cv2
import pandas as pd

# ==== è¼‰å…¥ç’°å¢ƒè®Šæ•¸ ====
load_dotenv()
DATABASE_URL = os.getenv("DATABASE_URL")
#OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
# è¨­å®š Tesseract è·¯å¾‘ (Windows)
pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

# è¨­å®š Tesseract OCR é…ç½®
custom_config = r'--oem 3 --psm 6 -l chi_tra+eng'

# è¨­å®š OpenAI API
#openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)
genai.configure(api_key=GEMINI_API_KEY)

# SECTION_MAP
SECTION_MAPPING = {
    "A": "ä¿®èª²ç´€éŒ„",
    "B": "æ›¸é¢å ±å‘Š",
    "C": "å¯¦ä½œä½œå“",
    "D": "è‡ªç„¶ç§‘å­¸æ¢ç©¶",
    "E": "ç¤¾æœƒé ˜åŸŸæ¢ç©¶",
    "F": "é«˜ä¸­è‡ªä¸»å­¸ç¿’è¨ˆç•«",
    "G": "ç¤¾åœ˜æ´»å‹•ç¶“é©—",
    "H": "å¹¹éƒ¨ç¶“é©—",
    "I": "æœå‹™å­¸ç¿’ç¶“é©—",
    "J": "ç«¶è³½è¡¨ç¾",
    "K": "éä¿®èª²æˆæœä½œå“",
    "L": "æª¢å®šè­‰ç…§",
    "M": "ç‰¹æ®Šå„ªè‰¯è¡¨ç¾",
    "N": "å¤šå…ƒè¡¨ç¾ç¶œæ•´å¿ƒå¾—",
    "O": "é«˜ä¸­å­¸ç¿’æ­·ç¨‹åæ€",
    "P": "å­¸ç¿’å‹•æ©Ÿ",
    "Q": "æœªä¾†å­¸ç¿’è¨ˆç•«",
    "R":"ç§‘ç³»è‡ªè¨‚",
    "S":"ç§‘ç³»è‡ªè¨‚",
    "T":"ç§‘ç³»è‡ªè¨‚"
}

# ==== é€£æ¥ PostgreSQL ====
def get_db_connection():
    return psycopg2.connect(DATABASE_URL)

# ====æ‰¾åˆ°è³‡æ–™å¤¾ä¸­æ‰€æœ‰æª”æ¡ˆ====
def process_doc_folder(folder_path):
    all_files = []

    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.lower().endswith((".pdf", ".docx")):
                all_files.append(os.path.join(root, file))

    if not all_files:
        print("âŒ æ²’æœ‰æ‰¾åˆ° PDF æˆ– DOCX æª”æ¡ˆ")
        return

    print(f"ğŸ“‚ ç™¼ç¾ {len(all_files)} ä»½æª”æ¡ˆï¼Œé–‹å§‹è§£æ...")

    for file_path in all_files:
        filename = os.path.basename(file_path)
        university, department = extract_university_department(filename)
        #university = "åœ‹ç«‹é‡‘é–€å¤§å­¸"
        print(f"\nğŸš€ æ­£åœ¨è§£æ: {filename}")

        if filename.lower().endswith(".pdf"):
            text = parse_pdf_with_ocr(file_path)
        elif filename.lower().endswith(".docx"):
            text = parse_docx_with_ocr(file_path)
        else:
            continue

        print("ğŸ¤– ä½¿ç”¨ GPT åˆ†æå…§å®¹...")
        parsed_data = analyze_with_gpt(text, university, department)

        if parsed_data is None:
            print("âŒ GPT åˆ†æå¤±æ•—ï¼Œè·³éæ­¤æª”æ¡ˆ")
            continue

        try:
            parsed_data = json.loads(parsed_data)
        except json.JSONDecodeError as e:
            print(f"âŒ JSON è§£æéŒ¯èª¤: {e}")
            continue

        parsed_data["sections"] = merge_duplicate_sections(parsed_data["sections"])
        print(f"âœ… å®Œæˆï¼š{parsed_data['university']} - {parsed_data['department']}")

        sql_output = generate_sql(parsed_data)
        with open("parsed_data.sql", "a", encoding="utf-8") as f:
            f.write(sql_output + "\n")

        store_in_database(parsed_data)
        print("âœ… è³‡æ–™å·²å¯«å…¥è³‡æ–™åº«")
        
# ==== è§£æ PDF æª”æ¡ˆ ====
def parse_docx_with_ocr(docx_path):
    """è§£æWordæ–‡ä»¶ï¼ŒåŒ…å«æ–‡å­—å’Œåœ–ç‰‡å…§å®¹"""
    doc = Document(docx_path)
    full_text = []
    
    # æå–æ–‡å­—
    for para in doc.paragraphs:
        full_text.append(para.text)
    
    # æå–åœ–ç‰‡ä¸¦é€²è¡ŒOCR
    for rel in doc.part.rels.values():
        if "image" in rel.target_ref:
            try:
                image_data = rel.target_part.blob
                ocr_text = process_image(image_data)
                if ocr_text:
                    full_text.append(f"\n[åœ–ç‰‡æ–‡å­—å…§å®¹]:\n{ocr_text}")
            except Exception as e:
                print(f"âŒ Wordåœ–ç‰‡è™•ç†å¤±æ•—: {e}")
    
    return "\n".join(full_text)

def parse_pdf_with_ocr(pdf_path):
    """è§£æPDFæ–‡ä»¶ï¼ŒåŒ…å«æ–‡å­—ã€åœ–ç‰‡å’Œè¡¨æ ¼å…§å®¹"""
    extracted_text = ""
    
    # ä½¿ç”¨pdfplumberæå–è¡¨æ ¼
    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages):
            # æå–è¡¨æ ¼
            tables = page.extract_tables()
            if tables:
                extracted_text += f"\n[ç¬¬{page_num + 1}é è¡¨æ ¼]\n"
                for table in tables:
                    df = pd.DataFrame(table)
                    extracted_text += df.to_string(index=False, header=False) + "\n\n"
    
    # ä½¿ç”¨PyMuPDFæå–æ–‡å­—å’Œåœ–ç‰‡
    doc = fitz.open(pdf_path)
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        
        # æå–æ–‡å­—
        text = page.get_text()
        if text:
            extracted_text += text + "\n"
        
        # æå–åœ–ç‰‡ä¸¦é€²è¡ŒOCR
        image_list = page.get_images()
        for img_index, img in enumerate(image_list):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image_data = base_image["image"]
            
            # è™•ç†åœ–ç‰‡ä¸¦é€²è¡ŒOCR
            ocr_text = process_image(image_data)
            if ocr_text:
                extracted_text += f"\n[åœ–ç‰‡æ–‡å­—å…§å®¹ {page_num + 1}-{img_index + 1}]:\n{ocr_text}\n"
    
    doc.close()
    
    # å¾Œè™•ç†ï¼šç§»é™¤é‡è¤‡å…§å®¹å’Œæ•´ç†æ ¼å¼
    lines = extracted_text.split('\n')
    unique_lines = []
    seen = set()
    for line in lines:
        cleaned_line = line.strip()
        if cleaned_line and cleaned_line not in seen:
            seen.add(cleaned_line)
            unique_lines.append(line)
    
    return '\n'.join(unique_lines)

# ==== å¾æª”åæå–å­¸æ ¡ & å­¸ç³» ====
def extract_university_department(filename):
    """ å¾æª”åè§£æå­¸æ ¡èˆ‡å­¸ç³»åç¨± """
    base_name = os.path.basename(filename).replace(".pdf", "")
    parts = base_name.split("_")

    if len(parts) >= 2:
        university = parts[0].strip()
        department = parts[1].strip()
    else:
        university = "æœªçŸ¥"
        department = "æœªçŸ¥"

    return university, department

# ==== ä½¿ç”¨ GPT è§£æå…§å®¹ ====
def generate_department_features(text, university, department):
    """ä½¿ç”¨ LLM ç”Ÿæˆå­¸ç³»ç‰¹è‰²"""
    prompt = f"""
    ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å­¸ç³»åˆ†æå°ˆå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹å…§å®¹ï¼Œåˆ†æä¸¦ç¸½çµå‡ºè©²å­¸ç³»çš„ç‰¹è‰²ï¼š

    å­¸æ ¡ï¼š{university}
    å­¸ç³»ï¼š{department}

    å…§å®¹ï¼š
    {text}

    è«‹å¾ä»¥ä¸‹å¹¾å€‹é¢å‘åˆ†æä¸¦ç”Ÿæˆå­¸ç³»ç‰¹è‰²ï¼š
    1. èª²ç¨‹èˆ‡å­¸ç¿’é‡é»
    2. æœªä¾†ç™¼å±•æ–¹å‘
    3. æ‰€éœ€èƒ½åŠ›èˆ‡ç‰¹è³ª
    4. ç‰¹è‰²å„ªå‹¢

    è«‹ç”¨ç°¡æ½”çš„æ–‡å­—æè¿°ï¼Œä¸è¦è¶…é300å­—ã€‚æ ¼å¼è¦æ±‚ï¼š
    1. ä¸è¦ä½¿ç”¨ç·¨è™Ÿæˆ–æ¨™é¡Œ
    2. ä»¥æµæš¢çš„æ®µè½å½¢å¼å‘ˆç¾
    3. ç¢ºä¿å…§å®¹å…·é«”ä¸”æœ‰åƒ¹å€¼
    4. é¿å…ç©ºæ³›çš„æè¿°
    """

    try:
        gemini_model = genai.GenerativeModel("gemini-2.0-flash")
        response = gemini_model.generate_content(
            prompt,
            generation_config={"temperature": 0.7}
        )
        
        if not response or not hasattr(response, "candidates"):
            raise ValueError("Gemini API å›æ‡‰ç‚ºç©º")
        
        features = response.candidates[0].content.parts[0].text.strip()
        return features if features else "ç„¡æ³•ç”Ÿæˆå­¸ç³»ç‰¹è‰²"

    except Exception as e:
        print(f"âŒ å­¸ç³»ç‰¹è‰²ç”Ÿæˆå¤±æ•—: {e}")
        return "ç„¡æ³•ç”Ÿæˆå­¸ç³»ç‰¹è‰²"

def analyze_with_gpt(text, university, department):
    """ä½¿ç”¨ GPT åˆ†ææ–‡ä»¶å…§å®¹"""
    # å…ˆç”Ÿæˆå­¸ç³»ç‰¹è‰²
    department_features = generate_department_features(text, university, department)
    print(f"\nğŸ¯ ç”Ÿæˆçš„å­¸ç³»ç‰¹è‰²ï¼š\n{department_features}\n")

    # åœ¨æç¤ºä¸­åŠ å…¥å°è¡¨æ ¼çµæ§‹çš„ç‰¹åˆ¥èªªæ˜
    prompt = f"""
    ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡æª”åˆ†æå·¥å…·ã€‚è«‹æ ¹æ“šä»¥ä¸‹å…§å®¹æå–ä¸¦æ ¼å¼åŒ–ç‚ºJSONæ ¼å¼ï¼š
    
    ç‰¹åˆ¥æ³¨æ„ï¼š
    1. å…§å®¹åŒ…å«è¡¨æ ¼å½¢å¼çš„å¯©æŸ¥é‡é»é …ç›®ï¼Œè«‹æ­£ç¢ºè§£æè¡¨æ ¼ä¸­çš„å°æ‡‰é—œä¿‚
    2. è¡¨æ ¼ä¸­çš„ç™¾åˆ†æ¯”æ¬Šé‡è«‹ä¿ç•™åœ¨section_nameä¸­
    3. ç¢ºä¿æ­£ç¢ºè­˜åˆ¥å„å€‹é …ç›®çš„å°æ‡‰é—œä¿‚
    4. ä½¿ç”¨æä¾›çš„å­¸ç³»ç‰¹è‰²
    
    å­¸æ ¡åç¨±: {university if university != "æœªçŸ¥" else "è«‹å¾å…§å®¹æå–"}
    å­¸ç³»åç¨±: {department if department != "æœªçŸ¥" else "è«‹å¾å…§å®¹æå–"}
    å­¸ç³»ç‰¹è‰²: {department_features}
    
    å…§å®¹ï¼š
    {text}
    
    è«‹è¼¸å‡ºæ¨™æº–JSONæ ¼å¼ï¼š
    {{
        "university": "å­¸æ ¡åç¨±",
        "department": "å­¸ç³»åç¨±",
        "department_features": "å­¸ç³»ç‰¹è‰²",
        "sections": [
            {{
                "section_code": "",
                "section_name": "",
                "content": ""
            }}
        ]
    }}
    """
    
    try:
        gemini_model = genai.GenerativeModel("gemini-2.0-flash")
        response = gemini_model.generate_content(
            prompt, 
            generation_config={"temperature": 0.6}
        )
        
        if not response or not hasattr(response, "candidates"):
            raise ValueError("Gemini API å›æ‡‰ç‚ºç©º")
        
        content = response.candidates[0].content.parts[0].text.strip()
        
        if content.startswith("```json"):
            content = content[7:].strip()
        elif content.startswith("```"):
            content = content[3:].strip()

        if content.endswith("```"):
            content = content[:-3].strip()

        print("ğŸ”¹ GPT å›æ‡‰å…§å®¹ï¼š")
        print(content)

        if not content:
            raise ValueError("GPT å›æ‡‰ç‚ºç©º")

        return content

    except Exception as e:
        print(f"âŒ GPT åˆ†æéŒ¯èª¤: {e}")
        return None

# ==== åˆä½µå¤šç­†è³‡æ–™ ====
def merge_duplicate_sections(sections):
    merged = defaultdict(lambda: {"section_name": "", "content": ""})

    for section in sections:
        code = section["section_code"]
        name = section["section_name"]
        content = section["content"] if section["content"] is not None else ""  # é˜²æ­¢ None

        if merged[code]["section_name"] == "":
            merged[code]["section_name"] = name

        if merged[code]["content"]:
            merged[code]["content"] += "\n" + content.strip()
        else:
            merged[code]["content"] = content.strip()

    return [
        {"section_code": code, "section_name": value["section_name"], "content": value["content"]}
        for code, value in merged.items()
    ]
    
# ==== ç”Ÿæˆ SQL èªæ³• ====
def generate_sql(data):
    sql_statements = []
    # **1ï¸âƒ£ å…ˆè™•ç† `application_guidelines` çš„ SQL**
    for section in data["sections"]:
        sql = f"""
        INSERT INTO application_guidelines (university, department, section_code, section_name, content)
        VALUES ('{data["university"]}', '{data["department"]}', '{section["section_code"]}', 
                '{section["section_name"]}', '{section["content"].replace("'", "''")}')
        ON CONFLICT (university, department, section_code) DO UPDATE
        SET section_name = EXCLUDED.section_name,
            content = EXCLUDED.content;
        """
        sql_statements.append(sql.strip())

    # **2ï¸âƒ£ æ–°å¢ `department_features` çš„ SQL**
    department_features_sql = f"""
    INSERT INTO department_features (university, department, features)
    VALUES ('{data["university"]}', '{data["department"]}', '{data.get("department_features", "æœªæä¾›å­¸ç³»ç‰¹è‰²").replace("'", "''")}')
    ON CONFLICT (university, department) DO UPDATE
    SET features = EXCLUDED.features;
    """
    sql_statements.append(department_features_sql.strip())

    return "\n".join(sql_statements)

# ==== å„²å­˜åˆ°è³‡æ–™åº« ====
def store_in_database(data):
    conn = get_db_connection()
    cursor = conn.cursor()

    # **å„²å­˜å‚™å¯©æŒ‡å¼•**
    for section in data["sections"]:
        cursor.execute("""
            INSERT INTO application_guidelines (university, department, section_code, section_name, content)
            VALUES (%s, %s, %s, %s, %s)
            ON CONFLICT (university, department, section_code) DO UPDATE
            SET section_name = EXCLUDED.section_name,
                content = EXCLUDED.content;
        """, (
            data["university"], data["department"],
            section["section_code"], section["section_name"],
            section["content"]
        ))
        
     # **å„²å­˜å­¸ç³»ç‰¹è‰²**
    cursor.execute("""
        INSERT INTO department_features (university, department, features)
        VALUES (%s, %s, %s)
        ON CONFLICT (university, department) DO UPDATE
        SET features = EXCLUDED.features;
    """, (
        data["university"], data["department"],
        data.get("department_features", "æœªæä¾›å­¸ç³»ç‰¹è‰²")
    ))

    conn.commit()
    cursor.close()
    conn.close()

# ==== OCR ç›¸é—œå‡½æ•¸ ====
def extract_text_from_image(image):
    """å¾åœ–ç‰‡ä¸­æå–æ–‡å­—"""
    try:
        # ä½¿ç”¨ pytesseract é€²è¡Œ OCRï¼ŒæŒ‡å®šèªè¨€ç‚ºç¹é«”ä¸­æ–‡
        text = pytesseract.image_to_string(image, lang='chi_tra')
        return text.strip()
    except Exception as e:
        print(f"âŒ OCR è™•ç†å¤±æ•—: {e}")
        return ""

def process_image(image_data):
    """è™•ç†åœ–ç‰‡ä¸¦é€²è¡ŒOCR"""
    try:
        # å°‡åœ–ç‰‡æ•¸æ“šè½‰æ›ç‚ºPIL Image
        image = Image.open(io.BytesIO(image_data))
        
        # è½‰æ›ç‚ºOpenCVæ ¼å¼é€²è¡Œé è™•ç†
        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        
        # åœ–ç‰‡é è™•ç†
        # 1. è½‰æ›ç‚ºç°åº¦åœ–
        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
        
        # 2. è‡ªé©æ‡‰äºŒå€¼åŒ–
        binary = cv2.adaptiveThreshold(
            gray, 
            255, 
            cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
            cv2.THRESH_BINARY, 
            11, 
            2
        )
        
        # 3. é™å™ª
        denoised = cv2.fastNlMeansDenoising(binary)
        
        # 4. é‚Šç·£å¢å¼·
        kernel = np.array([[-1,-1,-1],
                         [-1, 9,-1],
                         [-1,-1,-1]])
        sharpened = cv2.filter2D(denoised, -1, kernel)
        
        # è½‰å›PIL Imageæ ¼å¼
        processed_image = Image.fromarray(sharpened)
        
        # é€²è¡ŒOCRï¼Œä½¿ç”¨è‡ªå®šç¾©é…ç½®
        text = pytesseract.image_to_string(processed_image, config=custom_config)
        
        # æå–è¡¨æ ¼
        try:
            tables = pytesseract.image_to_data(processed_image, output_type=pytesseract.Output.DATAFRAME, config=custom_config)
            # éæ¿¾å‡ºæœ‰æ•ˆçš„è¡¨æ ¼æ•¸æ“š
            valid_tables = tables[tables['conf'] > 60]
            if not valid_tables.empty:
                text += "\n\n[è¡¨æ ¼å…§å®¹]\n"
                for _, row in valid_tables.iterrows():
                    if str(row['text']).strip():
                        text += f"{row['text']} "
        except Exception as e:
            print(f"âš ï¸ è¡¨æ ¼æå–å¤±æ•—: {e}")
        
        return text.strip()
    except Exception as e:
        print(f"âŒ åœ–ç‰‡è™•ç†å¤±æ•—: {e}")
        return ""

# ==== ä¸»ç¨‹å¼ ====
def main(pdf_path):
    
    university, department = extract_university_department(pdf_path)
    university ="ä¸­åœ‹æ–‡åŒ–å¤§å­¸"
    #department =""
    print(university, department)
    
    print("ğŸ“‚ è§£æ PDF ä¸­...")
    text = parse_pdf_with_ocr(pdf_path)
    #print(text)

    print("ğŸ¤– ä½¿ç”¨ GPT åˆ†æå…§å®¹...")
    parsed_data = analyze_with_gpt(text, university, department)

    if parsed_data is None:
        print("âŒ è§£æå¤±æ•—ï¼Œè«‹æª¢æŸ¥ GPT API æˆ– PDF å…§å®¹")
        exit(1)
        
    try:
        parsed_data = json.loads(parsed_data)
    except json.JSONDecodeError as e:
        print(f"âŒ JSON è§£æéŒ¯èª¤: {e}")
        print("âš ï¸ GPT åŸå§‹å›æ‡‰å…§å®¹å¦‚ä¸‹ï¼ˆå¯èƒ½æ ¼å¼ä¸æ­£ç¢ºæˆ–ä¸å®Œæ•´ï¼‰:")
        print(parsed_data)
        return
    
    # åˆä½µé‡è¤‡ section_code çš„å…§å®¹
    parsed_data["sections"] = merge_duplicate_sections(parsed_data["sections"])

    print(f"\nâœ… è§£æå®Œæˆï¼Œå­¸æ ¡ï¼š{parsed_data['university']}ï¼Œå­¸ç³»ï¼š{parsed_data['department']}")
    print(f"ğŸ“Œ å…±è§£æ {len(parsed_data['sections'])} ç­†è³‡æ–™ï¼š")

    # é¡¯ç¤ºè§£æçš„å…§å®¹
    for section in parsed_data["sections"]:
        print(f"\nğŸ“ {section['section_code']} - {section['section_name']}")
        print(f"ğŸ“– å…§å®¹: {section['content']}")

    # ç”¢ç”Ÿ SQL
    sql_output = generate_sql(parsed_data)
    sql_file = "parsed_data.sql"

    with open(sql_file, "w", encoding="utf-8") as f:
        f.write(sql_output)

    print(f"\nâœ… SQL èªæ³•å·²å­˜è‡³ `{sql_file}`ï¼Œå¯æ‰‹å‹•åŸ·è¡Œä»¥é©—è­‰è³‡æ–™ã€‚")

    # å­˜å…¥è³‡æ–™åº«
    print("ğŸ“¡ å„²å­˜åˆ° PostgreSQL...")
    store_in_database(parsed_data)
    print("âœ… è³‡æ–™æˆåŠŸå­˜å…¥ PostgreSQLï¼")

if __name__ == "__main__":
    folder_path = r"D:\lab\å€‹äººç”³è«‹å‚™å¯©è³‡æ–™AIè¼”å°ç³»çµ±\PDF\ä¸­åœ‹æ–‡åŒ–å¤§å­¸"  # è¨­å®šç›®æ¨™è³‡æ–™å¤¾
    process_doc_folder(folder_path)