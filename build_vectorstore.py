import os
import fitz  # PyMuPDF
import pickle
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# è®€å– .env è£¡çš„ OPENAI_API_KEY
load_dotenv()
vectorstore = FAISS.load_local("db/faiss_store", OpenAIEmbeddings())

# å»ºç«‹å‘é‡è³‡æ–™åº«
def process_pdfs_to_vectorstore(pdf_folder: str, output_path: str):
    embeddings = OpenAIEmbeddings()
    documents = []
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

    for root, _, files in os.walk(pdf_folder):
        for filename in files:
            if filename.lower().endswith(".pdf"):
                pdf_path = os.path.join(root, filename)
                doc = fitz.open(pdf_path)
                full_text = ""
                for page in doc:
                    full_text += page.get_text()

                if not full_text.strip():
                    print(f"âš ï¸ {filename} æ²’æœ‰æœ‰æ•ˆæ–‡å­—ï¼Œå·²ç•¥é")
                    continue

                chunks = splitter.split_text(full_text)
                if not chunks:
                    print(f"âš ï¸ {filename} åˆ†æ®µå¾Œç‚ºç©ºï¼Œå·²ç•¥é")
                    continue

                name_parts = filename.replace(".pdf", "").split("_")
                school = name_parts[0] if len(name_parts) > 0 else "æœªçŸ¥å­¸æ ¡"
                department = name_parts[1] if len(name_parts) > 1 else "æœªçŸ¥ç§‘ç³»"

                metadata = {
                    "school": school,
                    "department": department,
                    "source_pdf": filename,
                    "source_path": os.path.relpath(pdf_path, pdf_folder)
                }

                for chunk in chunks:
                    documents.append(Document(page_content=chunk, metadata=metadata))


    print(f"ğŸ“„ å…±å»ºç«‹ {len(documents)} æ®µæ–‡å­— chunk")

    if not documents:
        print("âŒ ç„¡ä»»ä½•æœ‰æ•ˆè³‡æ–™ï¼Œæœªå»ºç«‹å‘é‡åº«")
        return

    vectorstore = FAISS.from_documents(documents, embeddings)

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    vectorstore.save_local(output_path)  # output_path å»ºè­°ç”¨è³‡æ–™å¤¾åç¨±

    print(f"âœ… å‘é‡åº«å·²å„²å­˜è‡³ {output_path}")

# åŸ·è¡Œ
if __name__ == "__main__":
    process_pdfs_to_vectorstore("data", "db/faiss_store")
